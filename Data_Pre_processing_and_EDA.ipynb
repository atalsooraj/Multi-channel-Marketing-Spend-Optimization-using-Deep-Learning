{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Pre-processing and EDA.ipynb",
      "provenance": [],
      "mount_file_id": "1_osXvuiHgdnUftaf80GdaD7ZxhNEKAw0",
      "authorship_tag": "ABX9TyN5si+GsyJuTWNHjcD5QC65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atalsooraj/Multi-channel-Marketing-Spend-Optimization-using-Deep-Learning/blob/main/Data_Pre_processing_and_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IKt8dw75Crd"
      },
      "source": [
        "data_file = '/content/drive/MyDrive/606 Capstone/criteo_attribution_dataset.tsv.gz'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKpSA8sZ6Kem",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "736a1e52-4e65-461b-94b0-f3cb8df9964d"
      },
      "source": [
        "import pandas as pd\n",
        "# df_Criteo_Attribution = pd.read_csv(DATA_FILE, sep='\\t', compression='gzip')\n",
        "\n",
        "df0 = pd.read_csv(data_file, sep='\\t', compression='gzip')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-684fd6fa17b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# df_Criteo_Attribution = pd.read_csv(DATA_FILE, sep='\\t', compression='gzip')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22Yrj4xq6QiH"
      },
      "source": [
        "df0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQaZByOkfCy8"
      },
      "source": [
        "# df0[df0['uid']==7306395]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ahpwGMF8noe"
      },
      "source": [
        "len(df0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBV-v0mGgR5p"
      },
      "source": [
        "df0['campaign'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDlZ4zxZTkR7"
      },
      "source": [
        "df0['conversion'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbIoaWwF5TmY"
      },
      "source": [
        "df = df0.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ7FC89Mgn2Z"
      },
      "source": [
        "del df0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY1J1THseoI4"
      },
      "source": [
        "Step 1: Creating a new column 'jid' by concatenating uid and conversion id. We do this becasue we want to analyze entire customer journeys i.e sequence of events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yoNCE0J6Klv"
      },
      "source": [
        "#Concatenating uid and conversion_id since we want to analyze entire customer journeys(sequence of events)\n",
        "\n",
        "df['jid']= df['uid'].map(str)+ '_'+df['conversion_id'].map(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVDhoCXSgAEo"
      },
      "source": [
        "Step 2: Scaling the timestamp column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK9ClONn1Zbe"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGqbpf33wzxS"
      },
      "source": [
        "# Scaling timestamp and time_since_last_click columns \n",
        "minmaxscaler= MinMaxScaler()\n",
        "for column in ('timestamp','time_since_last_click'):\n",
        "  x= df[column].values.reshape(-1,1)\n",
        "  df[column +'_norm']= minmaxscaler.fit_transform(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0pbLOxIYKFE"
      },
      "source": [
        "df['campaign'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfsrzCyek48"
      },
      "source": [
        "Step 3: Randomly sampling 200 campaigns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC1zJAbO8Wrj"
      },
      "source": [
        "#Randomly sampling 200 campaigns\n",
        "import numpy as np\n",
        "camapaigns_shortlisted= np.random.choice(df['campaign'].unique(),200,replace=False)\n",
        "camapaigns_shortlisted[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPuA7bgCiOqE"
      },
      "source": [
        "Finding the set difference between original and shortlisted campaign id's. the result will give unwanted campaign id's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrajeGc85Vgj"
      },
      "source": [
        "not_reqd= np.setdiff1d(df['campaign'].values, camapaigns_shortlisted)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcgBlir3ifwN"
      },
      "source": [
        "Considering all campaign id's except those present in not required array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_CsttQP7hPE"
      },
      "source": [
        "df = df[~df['campaign'].isin(not_reqd)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgnudpJI8Zu5"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Hz9GmPByVK"
      },
      "source": [
        "df['campaign'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXBGdTimgBGX"
      },
      "source": [
        "# df_sampled= df[df['campaign'][j for j in range(len(df['campaign']))]== [i for i in camapaigns_shortlisted]]\n",
        "\n",
        "# df_sampled= df.loc[df['campaign'].apply(lambda x: x in camapaigns_shortlisted)]\n",
        "\n",
        "#Source-https://stackoverflow.com/questions/21738882/fast-pandas-filtering\n",
        "\n",
        "# df_sampled= df.loc[np.in1d(df['campaign'], camapaigns_shortlisted)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWnndeY4iuOh"
      },
      "source": [
        "Step 4: Counting the instances of each 'jid' and only considering theose journey id's having more than 1 occurence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8DdlAvsZIrq"
      },
      "source": [
        "grouped= df.groupby(['jid'])['uid'].count().reset_index(name='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PCa2NHI4EYR"
      },
      "source": [
        "#grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AYmT6aHVkWS"
      },
      "source": [
        "df2= df[df['jid'].isin(grouped[grouped['count'] >= 2]['jid'].values )]  #Only considering journey id's with more than 1 event. Here, we are indirectly removing jid's with only 1 occurence."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2MKj-arWFsR"
      },
      "source": [
        "#df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrFnKY2_hOlL"
      },
      "source": [
        "del df\n",
        "del not_reqd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL4x98CAWNrj"
      },
      "source": [
        "len(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCvZGmB5cMOa"
      },
      "source": [
        "df_0= df2[df2['conversion']==0]\n",
        "df_1= df2[df2['conversion']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V299M5xvcdcA"
      },
      "source": [
        "len(df_0), len(df_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc97G7_GokIo"
      },
      "source": [
        "del df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi59z_-njFXj"
      },
      "source": [
        "Step 5: Balancing the dataset: The number of convertd and non-converted events should be equal so that our model can have a balanced learning. We take all converted journeys and iteratively add non-converted events until the dataset is balanced. We do it this way because we want to balance the number of events but also want the whole journey."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3bBQiXYW-h_"
      },
      "source": [
        "df_0_jids= np.array_split(df_0['jid'].unique(),100* df_0.shape[0]/df_1.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oay5o7OkpJUk"
      },
      "source": [
        "df_0_sampled = pd.DataFrame(data=None, columns=df_0.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5MMc5Puk6C"
      },
      "source": [
        "for array in df_0_jids:\n",
        "  df_0_sampled = pd.concat([df_0_sampled, df_0[df_0.jid.isin(array)]])\n",
        "\n",
        "  if df_0_sampled.shape[0]> df_1.shape[0]:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhjpzlmOvOgL"
      },
      "source": [
        "df_final= pd.concat([df_0_sampled, df_1]).sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWvKJ0FhiEfu"
      },
      "source": [
        "del df_0,df_1,df_0_jids,df_0_sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQyAO2NovrRn"
      },
      "source": [
        "df_final['conversion'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JROhsQBWwuVm"
      },
      "source": [
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxdZ_EVM16pW"
      },
      "source": [
        "len(df_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx_6DVxnkhhq"
      },
      "source": [
        "Finding unique values in each categorical column. Here, cat 7 has way too many features so we are going to ignore this columns and use all the others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhX21qThHK3Q"
      },
      "source": [
        "df_final[df_final.columns[13:22]].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7XMFGhanre1"
      },
      "source": [
        "Creating a dictionary with all unique categorical variables as keys and assigning them values based on length of the dictionary when they are added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNH-ddEi2mbo"
      },
      "source": [
        "mapper= {}\n",
        "col_names= ['cat1','cat2','cat3','cat4','cat5','cat6','cat8','cat9']\n",
        "for i, col_name in enumerate(col_names):\n",
        "  for val in df_final[col_name].unique():\n",
        "    mapper[val*10 +i]=len(mapper)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZkT6oSsn5hq"
      },
      "source": [
        "Creating a new column containing the list of all the categories present in that corresponding rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WLaqkHL8zs5"
      },
      "source": [
        "df_ext = df_final.copy()\n",
        "df_ext['cats']= df_ext[col_names].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HnzlU809_I4"
      },
      "source": [
        "df_ext.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myheu-u1oDwE"
      },
      "source": [
        "Creating a function to do one hot encoding for each row value in campaign's column and update the encoded value in an array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5JSgKFb-9k1"
      },
      "source": [
        "def one_hot(values):\n",
        "  v= np.zeros(len(mapper))\n",
        "  for i, val in enumerate(values):\n",
        "    mapped_val_id= mapper[val*10+i]\n",
        "    v[mapped_val_id]= 1\n",
        "  return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVOxaiPz_knc"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ0V-h8a-CJ3"
      },
      "source": [
        "df_ext['cats']= df_ext['cats'].map(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KofgP6HMC1GK"
      },
      "source": [
        "#df_ext['cats'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAZbVT52sk6c"
      },
      "source": [
        "Getting all unique values in the campaign column and creating a dictionary with keys as campaign id and values as the length of the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VTQdlcmJYVk"
      },
      "source": [
        "mapper= {}\n",
        "col_names= ['campaign']\n",
        "for i, col_name in enumerate(col_names):\n",
        "  for val in df_ext[col_name].unique():\n",
        "    mapper[val*10 +i]=len(mapper)\n",
        "\n",
        "\n",
        "df_ext['campaigns']= df_ext[col_names].values.tolist()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASS0VfzL-iO"
      },
      "source": [
        "df_ext.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6yghioQs-MN"
      },
      "source": [
        "using the one-hot function for campaign column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rLj0eTTL9cF"
      },
      "source": [
        "df_ext['campaigns']= df_ext['campaigns'].map(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxTopCL9tF4C"
      },
      "source": [
        "Sorting the obtained dataframe by normalized timestamp value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bBWsQQBMQTC"
      },
      "source": [
        "df_model= df_ext.sort_values(by=['timestamp_norm'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt0Vx3fdMcL_"
      },
      "source": [
        "df_model.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCh0W-lNZzYd"
      },
      "source": [
        "# df_model['campaigns'][94389]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vodsVBpeuuMz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX6G2ySQTThX"
      },
      "source": [
        "journey_counts= df_model.groupby(['jid'])['uid'].count().reset_index(name='count').groupby(['count']).count()\n",
        "counts= journey_counts.index\n",
        "no_of_jids= journey_counts.values "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8PBySREUuPA"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkQI6OibVCZ3"
      },
      "source": [
        "plt.plot(counts,no_of_jids)\n",
        "plt.ylim(0,20)\n",
        "plt.xlim(0,200)\n",
        "plt.xlabel('Journey length (number of touchpoints)')\n",
        "plt.ylabel('No. of customers')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEl1drz4w6R8"
      },
      "source": [
        "From the above plot, it is clear that as the journey lenght increases, the number of jid's is decreasing. This is expected and makes sense. This re-affirms that we can develop a model for sequential data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIN2LbP_xwMc"
      },
      "source": [
        "Counting the number of events for each campaign_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh4c_U6kyr8d"
      },
      "source": [
        "counters= np.zeros(200)\n",
        "for campaign_one_hot in df_model['campaigns'].values:\n",
        "  campaign_id= np.argmax(campaign_one_hot)\n",
        "  counters[campaign_id]= counters[campaign_id]+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysdt-2d9zqRs"
      },
      "source": [
        "# counters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_bRwkvTyReW"
      },
      "source": [
        "To understand what counters represents, let us consider the first row from df_modelhaving an index 94389. When we apply the argmax function to an encoded array like in our case to the campaigns array, we get the position of value 1 in the encoded campaigns array i.e 86 here. Intitially, counters[86] will be 0(since we added 200 zeros). When a campaign's value in the encoded array is 86, it will increase the value of counter[86] by 1 and hence giving us the total number of occurences of a campaign in the entire dataset. So, now if we check the value of counters[86] we will get the count of the campaign 884761 which is 1368. So there are 1368 instances of the campaign 884761."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkAycl4wcA8b"
      },
      "source": [
        "np.argmax(df_model['campaigns'][94389])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "togSTilAzf0i"
      },
      "source": [
        "counters[86]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxJbvTIIW0Pr"
      },
      "source": [
        "df_converted= df_model[df_model['conversion']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IbFMNxpfhX3"
      },
      "source": [
        "# df_converted.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m7XnL8t0p12"
      },
      "source": [
        "len(df_converted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukQnsgzQ0iiY"
      },
      "source": [
        "df_converted['jid'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eZwW6Blz6sn"
      },
      "source": [
        "Counting the number of times a given campaign is the last touch before the conversion. here, when we groupby jid and timestamp_norm, we get 2 columns. One column is jid containing jid's  and the other containing the corresponding timestamp_norm value for the event. There may be multiple instances of jid's having multiple values of timestamp_norm values. The transform(max) function gives us the maximum value of timestamp_norm from among this sample of jid's. Finally, we check if this obtained max value is equal to the value in timestamp_norm column i.e if this was the last event and return a boolean value of either true or False along with the index value of timestamp_norm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFhZFc3cdJGw"
      },
      "source": [
        "index= df_converted.groupby(['jid'])['timestamp_norm'].transform(max) == df_converted['timestamp_norm']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzOlWNDK4BV3"
      },
      "source": [
        "index[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1H5a-QUkSSS"
      },
      "source": [
        "len(index.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V_w7icN5O5f"
      },
      "source": [
        "Taking all last touch arrays from campaigns column based on index of True values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vhszUaL35MT"
      },
      "source": [
        "# df_converted[index]['campaigns']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or7GTmGh5q3H"
      },
      "source": [
        "As done before, counting the number of times the event was last touch before conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlu6bFNsfCPh"
      },
      "source": [
        "counters_2= np.zeros(200)\n",
        "for campaign_one_hot in df_converted[index]['campaigns'].values:\n",
        "  campaign_id= np.argmax(campaign_one_hot)\n",
        "  counters_2[campaign_id]= counters_2[campaign_id]+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50txaK_i5143"
      },
      "source": [
        "Now, we have obtained the number of occurences for a campaign (counters) and number of times that campaign was last touch(counters). The ratio between the number of journeys in which a given campaign is the last event and the total number of events for the same campaign gives us the attribution weight (which is return per impression)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LYjLi9rgkFk"
      },
      "source": [
        "attributions= counters_2/counters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h5AW0Vil7TS"
      },
      "source": [
        "campaign_idx= range(0,200)\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "ax = fig.add_subplot(111)\n",
        "plt.bar( range(len(attributions[campaign_idx])), attributions[campaign_idx], label='LTA' )\n",
        "plt.xlabel('Sampled 200 campaigns')\n",
        "plt.ylabel('Return per impression (attribution)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coIAwc3tjXgb"
      },
      "source": [
        "del camapaigns_shortlisted, data_file, df_converted, df_ext, df_final, index, mapper, minmaxscaler, one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_pYdMG2mTSU"
      },
      "source": [
        "del attributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGZIm9O0kQ1C"
      },
      "source": [
        " del campaign_id,campaign_idx, campaign_one_hot, col_name, col_names, column, counters, counters_2, counts,grouped, i, journey_counts, no_of_jids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KePbr5-qOV_"
      },
      "source": [
        "df_model.to_csv('Dataframe_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ZiHSOqmYkt"
      },
      "source": [
        "# def features_for_logistic_regression(df):\n",
        "\n",
        "#     def pairwise_max(series):\n",
        "#         return np.max(series.tolist(), axis = 0).tolist()\n",
        "    \n",
        "#     aggregation = {                      # aggregation specification for each feature\n",
        "#         'campaigns': pairwise_max,\n",
        "#         'cats': pairwise_max,\n",
        "#         'click': 'sum',\n",
        "#         'cost': 'sum',\n",
        "#         'conversion': 'max'\n",
        "#     }\n",
        "    \n",
        "#     df_agg = df.groupby(['jid']).agg(aggregation)\n",
        "    \n",
        "#     df_agg['features'] = df_agg[['campaigns', 'cats', 'click', 'cost']].values.tolist()\n",
        "    \n",
        "#     return (\n",
        "#         np.stack(df_agg['features'].map(lambda x: np.hstack(x)).values),\n",
        "#         df_agg['conversion'].values\n",
        "#     )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9BokFCHn98V"
      },
      "source": [
        "# x,y = features_for_logistic_regression(df_model)\n",
        "# print(np.shape(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ3qAs32odX-"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}